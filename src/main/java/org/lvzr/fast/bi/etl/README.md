

###ETL是数据抽取（Extract）、清洗（Cleaning）、转换（Transform）、装载（Load）的过程

###ETL工具
http://blog.itpub.net/7868752/viewspace-1060666/

###ETL 浅谈
http://blog.csdn.net/adparking/article/details/48949905


###ETL的实现有多种方法，常用的有三种。
一种是借助ETL工具(如Oracle的OWB、SQL Server 2000的DTS、SQL Server2005的SSIS服务、Informatic等)实现，
一种是SQL方式实现，另外一种是ETL工具和SQL相结合。前两种方法各有各的优缺点，借助工具可以快速的建立起ETL工程，
	屏蔽了复杂的编码任务，提高了速度，降低了难度，但是缺少灵活性。SQL的方法优点是灵活，提高ETL运行效率，
	但是编码复杂，对技术要求比较高。
第三种是综合了前面二种的优点，会极大地提高ETL的开发速度和效率。


目前，ETL工具的典型代表有：Informatica、Datastage、OWB、微软DTS、Beeload、Kettle……
开源的工具有eclipse的etl插件。cloveretl.

主流ETL产品：
Ascential公司的Datastage（Datastage在2005年被IBM收购）
Informatica公司的Powercenter
NCR Teradata公司的ETL Automation（一套ETL框架、主要关注“抽取”）。

ETL工具有：
OWB(Oracle Warehouse Builder)
ODI(Oracle Data Integrator)
Informatic PowerCenter（Informatica公司）
AICloudETL、
DataStage（Ascential公司）
Repository Explorer
Beeload、Kettle
DataSpider
ETL Automation（NCR Teradata公司）
Data Integrator（Business Objects公司）
DecisionStream（Cognos公司）

###ETL工具的目标
可视化配置
抽取接口高可扩展，清洗规划高可扩展，转换规则高可扩展，目标接口高可扩展，灵活组装与配置
避免底层重复编码，做到ETL过程稳定性
将来可移植到分布式,负载均衡，资源充分利用
预处理机资源负载均衡情况
hdfs分布试spark数据批量ETL
流式日志收集kafka,flume,spark流式分布式ETL





-------------------------------------------------------------------------------------------------------
#架构设计
功能：关联、转换、增量、记录数限制、调度配置、大并发、多线程、过程监控、处理报告、报告通知、成功备份，失败备份，未处理标记
日志：执行日志、错误日志、总体日志、警告发送
识别标识出不合法数据以供确认

可批量可增量
内存存放规则，可视化配置，底层编码高可扩展
元数据管理(mogodb、hbase、solr、redis、memcached、database、file)
元数据的典型表现为对象的描述，即对数据库、表、列、列属性（类型、格式、约束等）以及主键/外部键关联等等的描述
而元数据对于ETL的集中表现为：定义数据源的位置及数据源的属性、确定从源数据到目标数据的对应规则、确定相关的业务逻辑、
在数据实际加载前的其他必要的准备工作，等等，它一般贯穿整个数据仓库项目，而ETL的所有过程必须最大化地参照元数据，这样才能快速实现ETL。
**ETL生命周期化，部件化，流程化，流程阶段可动态配置，部件可自由组装配置，插件化可扩展性，各阶段可底层灵活编码扩展**
ETL各部件层次平等或可嵌套
可配置式插件，可编码式插件，部件式插件，过程式插件

-----抽取-------------------------------------
抽取E：(可扩展的接口适配器，高度抽象)，(需有格式规则，灵活易用的的源元数据<可永久可临时>)，要分层，可固定，可适配
一次几条记录或游标
txt、xml、json、zip、database、xls、csv、mq

-----规则处理---------------------------------
转换前清洗C：
空值补缺原则(中值、平均值、默认值)，时间格式，数值格式，数据验证(正则表达式)，去重规则，去除不完整数据，去除错误不合法数据

转换T：
不一致数据归一化
数据粒度的转换
商务规则的计算

转换T：(源元数据映射到目标元数据，可映射到多目标源),由源元数据到目标元数据的映射，实现数据的转换
一行对多行，多行对多行，一行分多片，多行合成一行，字段拆分，字段合并，灵活的字符串截取
总之，设计时要有行映射规则，字段映射规则
特定情况的映射，应该自动实现清洗，减小清洗C配置工作

转换后清洗C：

-----加载-------------------------------------
加载L：(可扩展的接口适配器，高度抽象)、(目标源数据)
mogodb、hbase、solr、redis、memcached、database、filed





----------------------------------------------------------------------------------------------------
###支持完整的数据集成生命周期
数据集成平台必须支持数据集成生命周期中的所有五个关键步骤：访问、发现、 清洗、集成和交付。

第 1 步：访问 大多数机构的数据存储在数千个位置，不只限于企业内部，还存放在防火墙外的业务合作伙伴或 SaaS 供应商的“云”中。无论何种来源或结构，
          所有数据都必须可以接受访问。必须从隐秘的大型主机系统、关系数据库、应用程序、XML、消息甚至从电子数据表之类的文档中提取数据。

第 2 步：发现数据源－ 特别是记录不详尽或来源未知 － 必须探查才能了解其内容和结构。需要推断数据中隐含的模式和规则。必须标记潜在的数据质量问题。

第 3 步：清洗 必须清洗数据以确保其质量、准确性和完整性。必须解决错误或疏漏问题。必须强制执行数据标准，并且对值进行验证。必须删除重复的数据条目。

第 4 步：集成 要跨越多个系统保持一致的数据视图，必须集成并转换数据， 以便协调不同系统在定义各种数据元素并使之结构化的方式上存在的差异。例如，
对于“客户盈利”，营销系统和财务系统可能具有完全不同的业务定义和数据格式，这些差异必须得到解决。

第 5 步：交付 必须以适当的格式、在适当的时间将适当的数据交付给所有需要数据的应用程序和用户。交付数据的范围涵盖从支持实时业务运营的单个数据元素
           或记录到用于趋势分析和企业报告的数百万个记录。必须确保数据的高可用性和交付安全性。





-----------------------------------------------------------------------------------------------------
###ETL体系结构图
Design manager 提供一个图形化的映射环境，让开发者定义从源到目标的映射关系、转换、处理流程。设计过程的各对象的逻辑定义存储在一个元数据资料库中。

Meta data management 提供一个关于ETL设计和运行处理等相关定义、管理信息的元数据资料库。ETL引擎在运行时和其它应用都可参考此资料库中的元数据。

Extract 通过接口提取源数据，例如?ODBC、专用数据库接口和平面文件提取器，并参照元数据来决定数据的提取及其提取方式。

Transform 开发者将提取的数据，按照业务需要转换为目标数据结构，并实现汇总。

Load 加载经转换和汇总的数据到目标数据仓库中，可实现SQL或批量加载。

Transport services 利用网络协议或文件协议，在源和目标系统之间移动数据，利用内存在ETL处理的各组件中移动数据。

Administration and operation 可让管理员基于事件和时间进行调度、运行、监测ETL作业、管理错误信息、从失败中恢复和调节从源系统的输出。




--------------------------------------------------------------------------------------------------------
###具体实现 
####实现ETL，首先要实现ETL转换的过程。它可以集中地体现为以下几个方面：
1、空值处理 可捕获字段空值，进行加载或替换为其他含义数据，并可根据字段空值实现分流加载到不同目标库。

2、规范化数据格式 可实现字段格式约束定义，对于数据源中时间、数值、字符等数据，可自定义加载格式。

3、拆分数据 依据业务需求对字段可进行分解。例，主叫号 861082585313-8148，可进行区域码和电话号码分解。

4、验证数据正确性 可利用Lookup及拆分功能进行数据验证。例如，主叫号861082585313-8148，进行区域码和电话号码分解后，
	可利用Lookup返回主叫网关或交换机记载的主叫地区，进行数据验证。

5、数据替换 对于因业务因素，可实现无效数据、缺失数据的替换。

6、Lookup 查获丢失数据 Lookup实现子查询，并返回用其他手段获取的缺失字段，保证字段完整性。

7、建立ETL过程的主外键约束 对无依赖性的非法数据，可替换或导出到错误数据文件中，保证主键唯一记录的加载。


####为了能更好地实现ETL，笔者建议用户在实施ETL过程中应注意以下几点：
第一，如果条件允许，可利用数据中转区对运营数据进行预处理，保证集成与加载的高效性；

第二，如果ETL的过程是主动“拉取”，而不是从内部“推送”，其可控性将大为增强；

第三，ETL之前应制定流程化的配置管理和标准协议；

第四，关键数据标准至关重要。目前，ETL面临的最大挑战是当接收数据时其各源数据的**异构性和低质量**。以电信为例，A系统按照统计代码管理数据，
B系统按照账目数字管理，C系统按照语音ID管理。当ETL需要对这三个系统进行集成以获得对客户的全面视角时，这一过程需要复杂的匹配规则、
名称/地址正常化与标准化。而ETL在处理过程中会定义一个关键数据标准，并在此基础上，制定相应的数据接口标准。





---------------------------------------------------------------------------------------
###传统的 ETL 和 ELT 理念的优点和局限性
https://www.ibm.com/developerworks/cn/data/library/bd-hivetool/

多年来，ETL 技术和工具几乎完全没有变化，尤其在数据仓库方面。工具已经有所改进，但方法大体保持不变。
您从各种来源提取数据，运行一组脚本或 ETL 工作流来转换该数据，然后将其加载到一个星型模式或半标准化的数据仓库或主数据管理系统中。

鉴于这些限制，人们开始寻找解决方法，比如在本地数据库中存储数据。部门建立了自己的孤岛和数据集市，突然之间，主数据成为了
一种有趣的概念，而不是现实。数据并不是集成的数据。销售、营销和财务团队都有不同的数据。数字和仪表板是不可靠的，不值
得信任。显然，ETL 不能容纳大数据。




对于文件类型数据源(.txt,，xls)，可以培训业务人员利用数据库工具将这些数据导入到指定的数据库，然后从指定的数据库抽取。
或者可以借助工具实现，如SQL SERVER 2005 的SSIS服务的平面数据源和平面目标等组件导入ODS中去。


###数据的清洗转换

　　一般情况下，数据仓库分为ODS、DW两部分，通常的做法是从业务系统到ODS做清洗，将脏数据和不完整数据过滤掉，再从ODS到DW的过程中转换，
进行一些业务规则的计算和聚合。

###1、数据清洗

　　数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是否过滤掉还是由业务单位修正之后再进行抽取。
不符合要求的数据主要是有不完整的数据、错误的数据和重复的数据三大类。

　　A、不完整的数据，其特征是是一些应该有的信息缺失，如供应商的名称，分公司的名称，客户的区域信息缺失、业务系统中主表与明细表
不能匹配等。需要将这一类数据过滤出来，按缺失的内容分别写入不同Excel文件向客户提交，要求在规定的时间内补全。补全后才写入数据仓库。

　　B、错误的数据，产生原因是业务系统不够健全，在接收输入后没有进行判断直接写入后台数据库造成的，比如数值数据输成全角数字字符、
字符串数据后面有一个回车、日期格式不正确、日期越界等。这一类数据也要分类，对于类似于全角字符、数据前后有不面见字符的问题只能写SQL
的方式找出来，然后要求客户在业务系统修正之后抽取;日期格式不正确的或者是日期越界的这一类错误会导致ETL运行失败，这一类错误需要去业务系
统数据库用SQL的方式挑出来，交给业务主管部门要求限期修正，修正之后再抽取。

　　C、重复的数据，特别是维表中比较常见，将重复的数据的记录所有字段导出来，让客户确认并整理。

　　数据清洗是一个反复的过程，不可能在几天内完成，只有不断的发现问题，解决问题。对于是否过滤、是否修正一般要求客户确认;对于过滤掉的
数据，***写入Excel文件或者将过滤数据写入数据表，在ETL开发的初期可以每天向业务单位发送过滤数据的邮件，促使他们尽快的修正错误，同时也可以**
作为将来验证数据的依据。数据清洗需要注意的是不要将有用的数据过滤掉了，对于每个过滤规则认真进行验证，并要用户确认才行。


###2、数据转换

　　数据转换的任务主要是进行不一致的数据转换、数据粒度的转换和一些商务规则的计算。

　　A、不一致数据转换，这个过程是一个整合的过程，将不同业务系统的相同类型的数据统一，比如同一个供应商在结算系统的编码是XX0001，
而在CRM中编码是YY0001，这样在抽取过来之后统一转换成一个编码。

　　B、数据粒度的转换，业务系统一般存储非常明细的数据，而数据仓库中的数据是用来分析的，不需要非常明细的数据，一般情况下，会将业务系统
数据按照数据仓库粒度进行聚合。

　　C、商务规则的计算，不同的企业有不同的业务规则，不同的数据指标，这些指标有的时候不是简单的加加减减就能完成，这个时候需要在ETL中将
这些数据指标计算好了之后存储在数据仓库中，供分析使用。

###ETL日志与警告发送

　　1、ETL日志，记录日志的目的是随时可以知道ETL运行情况，如果出错了，出错在那里。

　　ETL日志分为三类。第一类是执行过程日志，是在ETL执行过程中每执行一步的记录，记录每次运行每一步骤的起始时间，影响了多少行数据，流水账
形式。第二类是错误日志，当某个模块出错的时候需要写错误日志，记录每次出错的时间，出错的模块以及出错的信息等。第三类日志是总体日志，只记录
ETL开始时间，结束时间是否成功信息。

　　如果使用ETL工具，工具会自动产生一些日志，这一类日志也可以作为ETL日志的一部分。

　　2、警告发送

　　ETL出错了，不仅要写ETL出错日志而且要向系统管理员发送警告，发送警告的方式有多种，常用的就是给系统管理员发送邮件，并附上出错的信息，方
便管理员排查错误。



--------------------------------------------------------------------------
###大数据时代下的数据集成（一）――ETL流程与技术架构
https://sanwen8.cn/p/3be5TwW.html

通过数据抽取，我们可以有效的解决刚才提到的问题1-3。但是需要特别说明的是，数据抽取并不仅仅是根据业务确定公共需求字段。
更涉及到从不同类型的数据库（Oracle、Mysql、DB2、Vertica等）、不同类型的文件系统（Linux、Windows、HDFS）、
以何种方式（数据库抽取、文件传输、流式）、何种频率（分钟、小时、天、周、月）、何种抽取方式（全量抽取、增量抽取）获取数据。
所以具体的实现也包含了大量的工作和技术难点。


数据转换就是处理抽取上来的数据中存在的不一致的过程。数据转换一般包括两类：
第一类：数据名称及格式的统一，即数据粒度转换、商务规则计算以及统一的命名、数据格式、计量单位等；针对问题4中的”厂商标识”字段，
将取值统一为“华为、中兴、阿郎、诺西、爱立信”。这样就需要对A省的该字段取值“1,2,3,4,5”根据映射关系进行数据转换；而对于问题5中
的”Iu接口配置带宽”字段，则将单位统一为Mbps，这样在对B省数据进行处理时，需要对取值除以1000000进行匹配。

第二类：数据仓库中存在源数据库中可能不存在的数据，因此需要进行字段的组合、分割或计算。以运营商获取的用户上网详单为例，需要根据用
户上网内容和流量类型确定用户使用的业务类型（流媒体、即时通信、下载、浏览等），生成相应字段。并对单个用户在单个小区的各类型业务流量、
次数、时间进行汇总统计数据转换实际上还包含了数据清洗的工作，需要根据业务规则对异常数据进行清洗，保证后续分析结果的准确性。问题6中“RNC标识”
为空的字段将会被清除。

传统ETL通常都是采用昂贵的ETL工具（***Datastage、SSIS***等）基于高性能的小型机完成。这种方式已经难以满足“大数据”时
代下TB甚至PB级的数据ETL需求，如何在有限的时间内，高效高质量的完成海量数据的ETL工作，对ETL技术的架构设计也提出了更高的要求。


###3.2 基于MapReduce的ETL技术架构
上图中给出了一个典型的基于MapReduce的ETL技术架构。 服务端主要包括元数据管理模块、执行引擎模块、数据访问模块。
元数据管理模块是系统的基础模块，它描述了系统中所有数据结构的定义，提供元数据存储、访问的服务。系统的其他模块通
过公共接口从元数据管理模块获得元数据信息。另外，元数据管理模块提供接口用来导入导出元数据。执行引擎模块是系统的核
心模块，又分为流程解析和流程执行两个模块。在流程解析模块，执行引擎获取执行流程的元数据信息，根据这些信息，生成相应
的工作流。流程执行模块完成从数据转换到数据解析的所有任务。数据访问模块提供公共的数据访问接口，它屏蔽了各种数据源之
间的差异，以一种统一的方式对数据进行查询、删除、修改。

在基于MapReduce的ETL技术框架下，开发人员只需要Map和Reduce两个函数进行数据转换的并行处理，并基于hadoop生态圈
所提供的API接口进行数据抽取和加载。这样可以提高开发效率，而且系统的并行处理能力也有成熟hadoop生态圈得以保证。
**但是MapReduce程序启动较为耗时，并不适用于数据的实时加载和入库，而且MapReduce作业流程的优化也需要投入大量的时间**。















