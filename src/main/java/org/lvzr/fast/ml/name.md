
###最大似然估计
https://www.zhihu.com/question/20447622

最大似然估计是一类方法的总称，包括了最小二乘法。例如：在线性回归问题中，假设误差服从高斯分布的前提下，对模型参数的最大似然估计就是最小二乘法。

似然估计的目的：即 通过 样本 确定某一分布中的参数。
原理： 通过若干次试验得到 某个参数值 能够使 样本出现的概率为最大，则称为最大似然估计。


说的通俗一点啊，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。例如：一个麻袋里有白球与黑球，
但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法：
 我假设我抽到黑球的概率为p,那得出8次黑球2次白球这个结果的概率为：P(黑=8)=p^8*（1-p）^2,现在我想要得出p是多少啊，很简单，使得P(黑=8)最
 大的p就是我要求的结果，接下来求导的的过程就是求极值的过程啦。可能你会有疑问，为什么要ln一下呢，这是因为ln把乘法变成加法了，且不会改
 变极值的位置（单调性保持一致嘛）这样求导会方便很多~同样，这样一道题：设总体X 的概率密度为 已知 X1,X2..Xn是样本观测值，求θ的极大似
 然估计这也一样啊，要得到 X1,X2..Xn这样一组样本观测值的概率是P{x1=X1,x2=X2,...xn=Xn}=

f(X1,θ)f(X2,θ)…f(Xn,θ)  然后我们就求使得P最大的θ就好啦，一样是求极值的过程，不再赘述。

###最大似然估计
现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数
估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测
的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），
得到最大似然估计值。

###最小二乘
找到一个（组）估计值，使得实际值与估计值的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的
，但绝对值在数学上求最小值比较麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平方加总之后的值最小，称为最小
二乘。“二乘”的英文为least square，其实英文的字面意思是“平方最小”。这时，将这个差的平方的和式对参数求导数，并取一阶导数为零，就是OLSE。
 
###极大似然估计
训练样本是在众多数据中被你首次观察到的样本，这样的样本发生概率应该尽可能大才能这么顺利一下子就作为训练样本被你观察到

###最小二乘法
在古汉语中“平方”称为“二乘”，“最小”指的是参数的估计值要保证各个观测点与估计点的距离的平方和达到最小。
 
###大数定理
大数定理就是样本均值在总体数量趋于无穷时依概率收敛于样本均值的数学期望（可不同分布）或者总体的均值（同分布）

###中心极限定理
中心极限定理就是一般在同分布的情况下,样本值的和在总体数量趋于无穷时的极限分布近似于正态分布.

大数定理说的是均值，摇骰子，摇100万次，均值趋近3.5；中心极限定理说的是分布，每次4颗骰子一起扔，每次都记下4颗的均值，扔100万次，这些均值服从正态分布

###大数定理
当样本容量逐渐增大，无限逼近总体容量时，样本均值也是无限逼近于总体均值（即教材上讲的是样本期望收敛于真实的期望）

###中心极限定理
对于N个相互独立的分布函数未知但期望和方差已定的随机变量，选样本容量为m抽样无数次，抽样的均值是满足正态分布的（比如说生产一箱货，
每箱重量的均值和方差确定但是任意一一箱的重量是随机的，把货不断装进很多货车上，这时候每箱货是满足正态分布的，定理描述的是随机变量的和）
 

大数定理是说样本足够大时，会接近期望，在样本无穷大时平均值是期望。（--》一个值）
中心极限定理说的是样本距离期望的涨跌偏差分布。（--》出现一种分布规律）

举个简单的例子，一滴水从高空落下，
经过一个随机分布的风向后，落在地上。大数定理指出，无论风向分布规律是什么，所有点距离垂直落下的点的距离应该等于
一个值，这个值就是期望中心极限定理指出，无论风向分布规律是什么，每个样本距离期望的位置的距离分布是符合正态分布的


###多重共线性的处理方法
http://bbs.pinggu.org/thread-3002418-1-1.html

##关于数据挖掘中“多重共线性”的确定方法（有图有真相）
http://blog.csdn.net/baimafujinji/article/details/49799409

回归分析是数据挖掘中最基本的方法，其中基于普通最小二乘法的多元线性回归要求模型中的特征数据不能存在有多重共线性，
否则模型的可信度将大打折扣。但是就是技术而言，如何确定模型中的各各特征之间是否有多重共线性呢？


###高斯马尔科夫定理
根据高斯马尔科夫定理，多重相关性并不影响最小二乘法估计量的无偏性和最小方差性，但是，虽然最小二乘估计量在所有线性估计量中是方差最小的，
但是这个方差都不一定小，而实际上可以找到一个有偏估计量，这个估计量虽然有较小的偏差，但它的精度却能够大大高于无偏的估计量。岭回归分析
就是根据这个原理，通过在正规方程中引入有**偏常熟二求**的回归估计量的


###岭回归
http://bbs.pinggu.org/thread-3166843-1-1.html
岭回归是加2范数罚的最小二乘回归

在处理数据做回归分析的时候，往往会遇到变量共线性的问题。一般处理它可以使用逐步回归，主成分分析等方法，还有一种叫做岭回归

岭回归可以用GAUSS来做，林光平的《计算计量经济学》里有程序，稍加修改就可以同时做岭回归和主成份回归了，很方便的。

岭回归分析是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，
以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。


###LASSO回归






 







