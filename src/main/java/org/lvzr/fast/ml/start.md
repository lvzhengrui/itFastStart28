机器学习与数据挖掘的学习路线图
http://blog.csdn.net/baimafujinji/article/details/49891221

应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。

说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。
同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且
在本文后面若提到这两个名词，我们所表示的意思是一致的。

但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、
HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!!
当然，学习机器学习看书是必备的，总不能靠冥想吧。。。

有的书介绍机器学习，会是这样一种思路：就是单独的一个一个的算法介绍，介绍个十几个，一本书的篇幅差不多也就完了。
李航博士的那本**《统计学习方法》**基本属于这种套路。当然，该书在国内是备受推崇的一本。客观上讲，国人写这方面的书很少，而李博士的著作也不像其
他那种大学教材一样东拼西凑，可谓良心之作。但就本书的思路来说，我认为：如果读者就单独的某一个算法想有所了解，参考该书应该会有收获。但系统化
上还是优化空间的，比如从一个算法到另外一个算法，之间的联系是什么，推动算法更新和升级的需求又在哪里？

另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚
类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大
阵营，所以按类别来介绍其中的算法也是一种很常见的思路。

这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益
的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而
不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。

当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、
大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）――如果你对其中的某些名词感到陌生，那
么就说明你尚**不具备深入开展数据挖掘算法学习的能力。你会发现到处都是门槛，很难继续进行下去**。


###第一条线路：
（基于普通最小二乘法的）简单线性回归 -> 线性回归中的新进展（岭回归和LASSO回归）->(此处可以插入Bagging和AdaBoost的内容)-> 
Logistic回归 ->支持向量机（SVM）->感知机学习 -> 神经网络（初学者可先主要关注BP算法）-> 深度学习

之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σxiβi，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们
之间的绳索了。其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。

基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多
重共线性，而且岭回归和LASSO就是对这些问题的修正。
当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人
们从哪里出发，才会如此设计出它们的。

###第二条路线：
K-means  -> EM  -> 朴素贝叶斯 -> 
贝叶斯网络 -> 隐马尔科夫模型（基本模型，前向算法，维特比算法，前向-后向算法） （->卡尔曼滤波）

这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是
绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。

而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因
所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以
作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确
实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容。

应用层面，R、MATLAB和Python都是做数据挖掘的利器，另外一个基于Java的免费数据挖掘工具是Weka，这个就只要点点鼠标，甚至不用编代码了。给一个软件界面的截图如下


可以参阅的书籍：
中文版（含翻译版）
1. 李航，统计学习方法
2. Pang-Ning Tan, Michael Steinbach  ， Vipin Kumar， 数据挖掘导论
3. Peter Harrington 机器学习实践

英文版
4. Stuart Russell,  Peter Norvig, Artificial Intelligence ： A Modern Approach（Third Edition）
5. Trevor Hastie， Robert Tibshirani，Jerome Friedman， The Elements of Statistical Learning：Data Mining, Inference, and Prediction


###极简读懂机器学习算法思维
（1）线性回归
回归最早是由高尔顿研究子女身高与父母身高遗传关系提出的，发现子女平均身高总是向中心回归而得名。其实“一分辛苦一分才”中就蕴含了线性回归算法
思想，比较简单表现出才能与辛苦是正比关系。另外，经常听说“成功=3分才能+6分机会+1分贵人帮”，这是标准的线性回归方程，其中成功是因变量；才能、
机会和贵人是自变量；而自变量前边的3、6和1是权重。

（2）K-聚类
中国有句古话是“物以类聚，人以群分”，其实已经蕴含了聚类算法的基本思想。比如说人，可以根据年龄分70后、80后、90后等；根据区域分北京、上海、
广东等；根据性别分男和女。人可以根据不同特征属性进行划分，而在聚类算法中是根据不同方式来计算两个事物的距离，如欧氏距离、皮尔森相似度等。
假如根据年龄可以每10年份划分为70后、80后、90后等，也可以根据成长周期分童年、少年、青年、中年和老年。因此特征刻度大小决定了群体的范围，这反
映在聚类算法中就是通过不同方法处理事物间距离，来确定事物属于哪个群体，如根据平均值、最大值等。
其中K值是表示划分群体的大小，如以区域为例，K=34，则划分为全国省份；K=7，则划分为东北，中原，华东，华北，华南，西北和西部等；K=2，则划分为南方和北方。

（3）K-邻近
中国还有句古话是“近朱者赤近墨者黑”，该句也蕴含了K-邻近算法的思想。比如判断一个人是否是有钱人，可以根据其最近联系的人群中，有钱人的比例来推
测。这就需要解决两个问题，一是如何确定最近联系人，二是如何计算有钱人比例。这反映在K-邻近算法中就是首先确定不同事物样本的距离，然后确定K值的
大小，根据K值内的有钱人占比，来预测未知用户的状态。
K值的大小将会直接决定预测结果，假如你有5个有钱人朋友，当K=8时，判定你为有钱人；但当K=12时，则判定你不是有钱人。因此在该算法中K的选择至关重要。

（4）朴素贝叶斯
“吃一亏长一智”反映了朴素贝叶斯算法思维，就是通过后验经验法，来对未知的预测。假如你经常买水果，发现10个青苹果里边8个都是酸的，而10个红苹
果里有7个都是甜的，这种经验告诉以后再挑选苹果时，红苹果7/10是甜的，青苹果2/10是甜的，如果你喜欢甜的，那就选红苹果吧。

（5）决策树
在婚恋相亲时经常被问到“你有车吗？你有房吗？你有钱吗？”，这和决策树的思维过程极其相似。决策树是由树枝，树叶，节点组成的树型结构，其中每
个节点就是一个问题或特征（如你有车吗？），每个树枝是问题的走向（如有），每个节点就是答案（相亲成功）。

（6）主成分分析
经常在网上看到两个字“干货”。那怎么定义“干货”，我觉得应该包括两方面：一是信息量大，二是没有废话。其实如何将“水货”制作成干货的过程，
与主成分分析有异曲同工之妙。“干货”能够使原文到达“短小精悍”，而主成分分析能够实现数据集降维，即用较少维度表示原有样本含有的信息，两则
都是通过其它语言或转变维度来表达原有信息。
“水货”变成“干货”就是将意思相近或相似的句子进行浓缩或提炼，也就是将“水货”里的的水分拧干；而主成分分析是根据样本集的协方差矩阵，通过
线性变换将原数据映射到新的坐标系统，并将差异性较大特征值的保留，以到达降维目的。

（7）随机森林
“三个臭皮匠赛过诸葛亮”与随机森林算法内核类似。随机森林是是由一棵棵的决策树构成的，每决策树的形成都是随机的，它可以避免单一决策树过拟合和偏向的毛病。
再以相亲为例，对相亲对象要求，你可能看重“有房”“有车”“有钱”；你妈看重“有房”“孝顺”；你爸看重“事业”“顾家”“有车”等。其实你们每个人
都是一个决策树，可根据自己判断标准决策出相亲对手是否“满意”，最后集合每个人的决策结果，来判断最后是否相亲成功。一个人相亲是决策树，全家人相亲
就是随机森林。
 相亲随机森林

（8）最大熵模型
“不要把鸡蛋放在一个篮子里”是最大熵模型比较朴素的说法，也反映了该算法的本质，就是对不确定的或未知的，尽量保持随机和均匀分布，能够将风险降到最
低。其实在生活中大家应该都不自觉的应用了该模型。比如，去年P2P较火的时候，很多人被其高收益吸引，但由于P2P鱼龙混杂，又担心跑路；因此采取比较保
险的举措，就是多投几家公司。
其实，熵是对无序状态的描述，而最大熵就是表示样本是均匀分布，可能性概率相同。

（9）AdaBoost
在学生时代，考试有个技巧就是构建自己的“错题本”，每次考试前都加强对“错题本”学习，通过不断强化“错题本”上题目，最终可能获得较高分数。其实
这个学习过程与AdaBoost是算法逻辑是相同的。
假设每次考试作为一次模型训练，每道题目作为一个样本，分数作为预测准确率，而“错题本”就是预测错误的样本；当再次进行预测训练考试的时候，
AdaBoost算法策略就是会对上次预测“错误的样本”加大权重，并以此不断迭代，通过多次训练，最后能够组合成一个较强的分类器（即考试高分）。

（10）关联规则
是否耳熟“我看你天赋异禀、骨骼惊奇，想来是百年难得一见的练武奇才”“贫道夜观天象，发现北斗星南移，天狼星耀青光，帝王星显现”等台词。其实
这里边就蕴含了关联规则，通过经验积累发现骨骼与练武，北斗星与帝王等之间关联。
“用生辰八字来算命”虽然被成为伪科学，但偶尔能算准，这是这么回事？用关联规则算法就容易解释，首先理解两个概念支持度和置信度。
支持度是指A（某生辰八字）和B（某命运）同时发生的占比，如某生辰对应某命运的人数占总人数比值；置信度是指A发生后B发生的概率，如某生辰中当官
的人数/某生辰总人数。如果置信度是100%，如果A发生，那么B一定发生。算命先生就将生辰和命运的置信度定为100%。
如果算命先生学过机器学习算法，就不会很肯定指出你将来一定当官，而是说你将来当官的支持度为20%，置信度为30%。


 
