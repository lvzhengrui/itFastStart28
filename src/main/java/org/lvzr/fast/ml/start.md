机器学习与数据挖掘的学习路线图
http://blog.csdn.net/baimafujinji/article/details/49891221

应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。

说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。
同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且
在本文后面若提到这两个名词，我们所表示的意思是一致的。

但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、
HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!!
当然，学习机器学习看书是必备的，总不能靠冥想吧。。。

有的书介绍机器学习，会是这样一种思路：就是单独的一个一个的算法介绍，介绍个十几个，一本书的篇幅差不多也就完了。
李航博士的那本《统计学习方法》基本属于这种套路。当然，该书在国内是备受推崇的一本。客观上讲，国人写这方面的书很少，而李博士的著作也不像其
他那种大学教材一样东拼西凑，可谓良心之作。但就本书的思路来说，我认为：如果读者就单独的某一个算法想有所了解，参考该书应该会有收获。但系统化
上还是优化空间的，比如从一个算法到另外一个算法，之间的联系是什么，推动算法更新和升级的需求又在哪里？

另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚
类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大
阵营，所以按类别来介绍其中的算法也是一种很常见的思路。

这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益
的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而
不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。

当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、
大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）――如果你对其中的某些名词感到陌生，那
么就说明你尚不具备深入开展数据挖掘算法学习的能力。**你会发现到处都是门槛，很难继续进行下去**。

第一条线路：
（基于普通最小二乘法的）简单线性回归 -> 线性回归中的新进展（岭回归和LASSO回归）->(此处可以插入Bagging和AdaBoost的内容)-> 
Logistic回归 ->支持向量机（SVM）->感知机学习 -> 神经网络（初学者可先主要关注BP算法）-> 深度学习

之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σxiβi，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们
之间的绳索了。其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。
基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多
重共线性，而且岭回归和LASSO就是对这些问题的修正。
当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人
们从哪里出发，才会如此设计出它们的。


第二条路线：
K-means  -> EM  -> 朴素贝叶斯 -> 
贝叶斯网络 -> 隐马尔科夫模型（基本模型，前向算法，维特比算法，前向-后向算法） （->卡尔曼滤波）

这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是
绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。
而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因
所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以
作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确
实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容。

应用层面，R、MATLAB和Python都是做数据挖掘的利器，另外一个基于Java的免费数据挖掘工具是Weka，这个就只要点点鼠标，甚至不用编代码了。给一个软件界面的截图如下


可以参阅的书籍：
中文版（含翻译版）
1. 李航，统计学习方法
2. Pang-Ning Tan, Michael Steinbach  ， Vipin Kumar， 数据挖掘导论
3. Peter Harrington 机器学习实践

英文版
4. Stuart Russell,  Peter Norvig, Artificial Intelligence ： A Modern Approach（Third Edition）
5. Trevor Hastie， Robert Tibshirani，Jerome Friedman， The Elements of Statistical Learning：Data Mining, Inference, and Prediction



